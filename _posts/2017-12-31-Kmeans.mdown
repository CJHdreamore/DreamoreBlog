---
layout: post
title: Kmeans笔记
date: 2017-12-31
categories: learning
tags: [sklearn]
author: 陈小耗
---

# Sklearn学习笔记
### Input data
1. 输入数据集，可以是各种形式的矩阵。比如最标准的二维矩阵：[n_samples,n_features]。
2. 也可以是这样的二维矩阵：[n_sample,n_sample]。对于这样的聚类函数，不是基于数据集的feature来聚类，而是根据相邻样本点的similarity来聚类，比如DBSCAN.

## Sklearn中的Kmeans
- **原理**

        1.Kmeans的方法聚出来的每一个类别是用它的中心点作为描述，相当于一个核心向量。

        2.Kmeans聚类的原理是最小化相同簇中不同样本间的距离（within-cluster），亦称为inertia,可以视为描述一个簇的聚合度（凝聚力）

- **局限性**

        1.它假设了每一个簇是凸的(convex），等方的（isotropic）。对于那些狭长的簇，或者有着非常规流型的簇，Kmeans就不太适用。（demo-1）

        2.如果以欧氏距离来作为样本间的度量，这个度量不是一个归一化度量(normalized metric)。样本的值越小，对达成优化目标越有利。同时在高维，会出现维数灾难的问题。所以对待高维问题，在使用Kmeans聚类之前作PCA是很重要的，它可以加快运算。（demo-2）

- **算法**
Kmeans通常有三步：

        初始化：设置初始质心集合，这里的选择方式有多种，比如：random；‘k-means++’(这种方式是指尽量让质心彼此间远离，有点类似于尽量增大不同类之间的距离）；还有一种确定式的以PCA确定的主成分来作为初始的质心。至于质心点的个数，当然是和预成簇的个数一致。

        第二步：将训练数据中的每一个样本分配给离它最近的质心。

        第三步： 更新centroids。因为现在centroid管辖的范围里不止它一个了，所以要取所有该簇中的样本求平均后的值来作为新的centroid。（于是质心移动了）。计算新的centroid与原centroid的差，如果小于预设的threshold，证明无需移动质心，聚类完成。否则返回第二步。

> - Kmeans等价于EM算法（有一个小的，所有值相等的对角协方差矩阵）


Kmeans总会收敛，虽然有可能收敛到一个局部最小值。这主要取决于**初始质心的选取**。由于这样的不确定性，通常要做多次不同初始化的Kmeans（e.g.n_init=10），再取其中聚类效果最好的一次（如何判断聚类效果，则需要度量），sklearn.cluster中的Kmeans正是这样做的。

***
- **Demo分析**
- **DEMO-1**：展示数据具有**何种分布**的时候，Kmeans的聚类效果比较好。

>     It scales well to very large n_samples,medium n_clusters

Demo1充分向我们展示了：

Kmeans依赖于对cluster number的预判，并且它对于**anisotropicaly** distributed data和**非均匀方差**的数据的聚类效果都**不好**。不过，它对于数据集的大小不敏感，所以才适合于大量样本的聚类吧。

**codes细节记录：**
1.用plt画出数据集的散点图时，直接呈现不同簇数据的方式是：

    plt.scatter(X[:,0],X[:,1],c=pred)

 将Kmeans模型预测得到的label直接用于c这个代表颜色的变量中。非常简洁优雅的一种呈现方式。

2.在生成不同大小的簇数据集时，有一种神奇的filter方式。

利用make_blobs函数返回的两个变量：第一个是一个二维的numpy数组，代表数据集；第二个则是每个样本对应的label。
为什么可以利用这样的代码来做filter呢？
    X[y == 0][:500]

用一个中间变量来取出y==0，发现这是一个有如下形式的列表：[T,F,T,...]

X[[T,F,T,...]将会取出所有为T位置上的样本，也就是取出了label为0的对应的那些样本。

**这个filter的技能我很陌生**，之所以可以用这种方法取出对应标记的样本，是因为make_blobs返回的X，y它们的行数是一致的。

3.为什么要设置random_state?

我终于知道为什么在代码中要设置random_state=某个常数
或者在代码一开始就写：np.random.seed(42) (demo-2)

- 如果没有设置随机数种子，那么我们每次run程序得到的结果都是不同的，原因在于我们是随机生成的原始训练数据。然而在实现聚类的过程中，我们可能会不断修改代码中模型的参数，如果每次使用的原始训练数据都不一致，那么我们就无法知道改变参数是否使这个模型变好了。

这就是为什么要设置random_state的原因。

***
2.**DEMO-2**：用手写数字（0-9）的图片作为训练集（高维,d=64），样本一共1792个。

K-means能否将代表不同数字的样本成功聚为10类呢？

- codes细节：

1.将得到的（n_samples=1792,n_features=64）的二维数组标准化

**为什么要标准化？K-means聚类前都需要标准化吗？标准化是为了让数据服从某种分布吗？**

2.kmeans中的interia指的是什么？

> The Kmeans algorithm clusters data by trying to separate samples in n groups of **equal variance**,minimizing a criterion known as the **inertia** or within-cluster sum-of-squares.

上面这段话中告诉我们“inertia”是一个准则，是kmeans的目标函数：一个簇中所有的样本
到其centroid点的距离的平方和。这就是inertia，Kmeans的聚类等价于找到一组centroid使在训练样本上inertia最小。代码中显示出具体的inertia值，说明是把k个簇的inertia都加起来了。

3.在代码中用了**6种**不同的度量方式来计算预测的label与真实的label之间的误差，从而反映该聚类模型的性能。

**为什么明明是无监督学习，就是不知道样本的真实label,还可以用这种方式来进行模型的评估呢？不是很矛盾吗？**

这涉及到，如何评价聚类结果的好坏。

周志华老师的西瓜书中说**“聚类不存在客观标准**”,但我们有时仍需要对其进行**性能度量**。

聚类的性能度量称为“有效性指标（validity index）”，有外部指标和内部指标之分。

- 外部指标：将聚类的结果和领域专家给出的划分作比较。
                   比如demo-2中的labels，就可以视为是外部给出的一个参考指标。
- 内部指标：不用任何参考指标，直接依据聚类结果来判定。

在sklearn中也有专门探讨"clustering performance evaluation"

关于聚类性能度量，详见我的另一篇总结
[聚类的性能度量]
(http://haoxiangdreamore.com/Cluster_performance/)







