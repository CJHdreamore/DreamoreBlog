---
layout: post
title: 神奇的核方法
author: 陈小耗
date: 2018-01-06
categories: learning
tags: [西瓜书][sklearn]
---

- **这篇博文是“核方法”的笔记、sklearn中运用kernel的一些算法，以及我对此的感悟。**

# 神奇的核方法

## 初见核函数

初识“核方法”是在对SVM的学习中，彼时我得到的概念是：

针对线性不可分的数据样本，我需要将样本映射到高维的特征空间中，在这个特征空间中使用线性分类器（常规构造的一个超平面），就可以实现对原本线性分类器不可分的样本的划分。

诚然如是，核方法这个名字其实掩盖了某些实质 —— 它的本质无非是将样本通过一个函数映射到了高维的特征空间。

它的诡异在于，这个进行特征映射的函数我们往往无法显示地写出来，**但是我们仍然有办法继续进行想要的计算**，而这就依赖于**核函数**.

所以应该认清的是：

   1.核函数**不是**把样本映射到高维特征空间的函数,这个函数往往是无法显式写出的，常用phi(·)表示。
   
   2.样本x_i和样本x_j在高维特征空间中的内积，假设用phi(x_i)^Tphi(x_j)表示。**恰好等价于在样本空间中用特征函数对（x_i,x_j）进行计算。**

- 可见核函数有两个输入：样本x_i和样本x_j，它代替了这两个样本在高维特征空间中的内积计算。


所以核函数仿佛我们设想的一种超能力：如果它确实存在的话，我们不必先把样本映射到高维特征空间，再做内积，而只需拿出这个核函数，在样本空间中对两个样本进行计算就好了。

可问题是：这样的超能力真的存在吗？我们如何获得它呢？

## 又见核函数

世界的奇妙就在于，它竟然真的告诉我们：核函数是存在的，并且它应该满足什么样的条件。

假设在样本空间中有m个样本:x_1,x_2,...,x_m

我们假设存在一个二输入的对称函数，k(·,·),它作用在这m个样本上，构成一个 m * m的核矩阵K。于是有以下定理：

> 只要一个对称函数所对应的核矩阵K半正定，那么k(·,·)就能作为一个核函数使用。也就是说，这个核函数必然对应了某个特征映射函数phi(·)能够将样本映射到高维的特征空间。

> “换言之，任何一个核函数都隐式地定义了一个称为[再生核希尔伯特空间]的特征空间。”

所以说，选择核函数，等价于选择一个**将样本映射到高维特征空间的函数**，它直接决定了能否成功对样本集实现非线性的划分。

## 从核函数到核方法

什么是核方法？

我愿意把核方法理解为：通过引入核函数实现线性学习器到非线性学习器的拓展。

学习器包含了：分类器，回归模型，降维方法，聚类方法。

- 核函数是如何出现在从线性到非线性的道路上的？


### 首先以**分类器**为对象来理解：


**step 1** 此刻是线性方程

> 分类就是要基于训练数据在样本空间中找到一个划分超平面


从最简单的构想出发，我们自然而然地将这个超平面用一个线性方程来进行表示。

用线性方程写出了特征空间中的超平面，下一步我们就要想，如何寻求方程中的参数：系数w和截距b。

**step 2** 此刻是优化问题

为了求解线性方程中的参数，我们来到了**优化**的领域。

以**SVM算法**为例，它将线性方程组的参数求解转化为优化问题的方法如下：

线性方程：f(x) = W_T * x + b,其中x,b都指向量，W_T是参数矩阵的转置


1.优化目标：最大间隔，间隔这个概念是由w参数来定义的；

2.优化约束：正确分类，这个约束不等式是基于**原线性方程**来表达的，等价于满足原线性方程。


**step3** 此刻是对偶优化

利用“拉格朗日乘子法”得到step2中优化问题的**对偶问题**。
- 注意了：在这个对偶问题的表示中，目标函数里出现了**样本内积**的形式！

这正是后续**核函数现身**的重要来源！！！(现在知道为什么要把原本的优化问题转化为其对偶问题来处理了吧.)


**step4** 此刻是分类器表示

终于求解对偶优化问题，我们能够得出模型的表示了。

在最终模型的表达式中，可以看到有**样本内积**的形式。换句话说：模型的最优解是通过**训练样本的内积**来展开的。

**step5** 此刻是高维特征空间中的线性方程

在step1中，对样本线性可分的情况我们写出了一个用线性方程表示的超平面。

如果样本是非线性可分的，我们选择把样本先映射到一个高维特征空间，在高维空间中写出线性方程表示的超平面。

这样做的依据是在西瓜书的P126页写道：

> 如果原始空间是有限维，那么一定存在一个高维特征空间使样本可分。

虽然我仍不知道它的底层依据是什么...姑且先承认这个statement。

于是我们得到了“**非线性可分样本的高维特征空间中的线性方程**”。

**step6** 此刻是优化问题——对偶优化——求解

既然它仍是一个线性方程，于是按相同的做法进行转化，最后发现！

此时解对偶优化，模型的最优解的表示中出现了：样本特征映射后的内积

而这，正是完全等价于我们核函数的操作啊！

于是，模型的最优解变成了训练样本的核函数的展开！

回答最初提出的问题： 核函数是怎么出现在线性到非线性问题的扩展中的？



 **线性可分样本**在SVM算法中的最优分类器，是在样本内积上的展开；

 **非线性可分样本**则是在特征空间中*伪样本*内积上的展开。

 计算样本在特征空间中的内积*等价于*样本在原空间中计算核函数。

 于是：

 **非线性可分样本**的最优分类器是在样本空间中核函数上的展开。


- 总结：


如果线性学习器的表示中没有**内积**这个形式，那么就无法用**核函数**来将其扩展为非线性的学习器。



### 让我们用**降维**来佐证：


**step 1** 线性方程的提出

最普通的PCA降维，是通过线性变化来把样本映射到一个新的坐标空间，再丢弃部分不重要的坐标轴，构成一个低维的子空间，从而实现降维。

因为**线性变化**的缘故，其实也就是一个线性方程，和SVM中超平面的线性方程本质是一样的。

**step 2** 转化为优化问题

在把线性方程转化为优化问题的这一步中：我们说，不同的学习器呈现出了不同的思路。

在SVM中，它根据定义的间隔（margin)概念(最大化分类间隔）和分类正确的约束构造出了优化问题。

而在PCA中，它可以基于"最近重构性"或者“最大可分性”导出同一个优化问题。用最大可解释方差来理解的话，它的优化目标是：投影后样本点的方差最大化，约束是变换矩阵是一个单位正交阵。

从这里可以体会到，PCA中没有利用样本中的标记信息（SVM优化问题的约束中利用了样本的标记），所以PCA更像是对样本做的**预处理**。

它很单纯，只是想要对样本进行降维，当然它也很有远见，因为它希望降维后能尽量有利于后续的任务（比如分类）。

幸运的是，PCA转化成的优化问题中，它的优化目标里就包含了样本内积的形式。这指示我们：

如果样本不能用线性变换的方式实现降维的话，没关系，我们可以先把样本映射到高维特征空间，再从高维特征空间里用线性变换来进行降维！

因为目标函数里有样本内积的形式，所以这时我们不惧，因为在非线性变换的情况下，我们用核函数就可以处理。

这就是**KPCA**的来源。

的确存在普通的线性变换无法对样本进行**正确**的降维的情况，西瓜书的P232的例子展示出了这一点。

核化真的非常棒了！


## 结语：

这篇笔记主要分析和总结了以下几个问题：

  1.核函数是什么？它不是对样本进行特征映射的函数，它是样本在高维特征空间做内积计算等价在样本空间对原样本进行计算的函数。

  2.核方法通过引入核函数实现了线性学习器到非线性学习器的扩展。

  3.之所以能用核函数实现线性到非线性的扩展，是因为学习问题等价的优化问题的目标函数中有一项*内积*表示。

  4.SVM和PCA在转化为优化问题时，通过**目标函数**和**约束**表现出了各自作为监督学习分类器和将为算法的特点。


  












