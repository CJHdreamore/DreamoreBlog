---
layout: post
title: 机器学习中的降维
author: 陈小耗
data: 2018-01-04
categories: learning
tags: [西瓜书,sklearn]
---
# 前言：这篇笔记是结合周志华老师的西瓜书以及sklearn的user guide学习**降维算法**的过程

***

## 为什么要讨论降维？

**西瓜书中是通过KNN（K-近邻算法）来引入的维数问题。**

- K-NN算法是一种懒惰学习（lazy learning)

在训练过程中，其实它只保存了样本。在测试时，KNN计算测试样本和每一个训练样本间的距离，在选定K的情况下，返回K个最近的训练样本用于分类或者回归。

K的选定是最后分类或者回归结果好坏的一个重要参数。

在书中讨论了K=1时，即最近邻分类器在分类时的泛化误差，一个简单推导后得出的结论是：

> 最简单的1NN的泛化错误率不超过贝叶斯最优分类器的错误率的两倍！！


然而，得到这个结论有一个前提假设：**对于任意的测试样本，总能在其任意近的范围内找到一个训练样本**

这个假设的通俗解释就是：我们从总体中采样得到的训练样本密度必须足够大！也就是所谓的**dense sample**。

实际中满足密采样是怎么一回事呢？

举个例子：想要在一个维度=1的测试样本(只有一个属性)附近0.001的距离范围内都能找到一个训练样本，那么就要求1000个样本点平均分布在归一化后的该属性的取值范围内。

倘若有更多的属性（样本是高维的）,那么所需的样本将会呈指数上升，比如维度=20，需要样本1000^20.更遑论之后还要对这些样本进行距离的计算。

也就是说：在高维情况下，会存在样本稀疏、距离计算困难这样的问题。这就是机器学习中的**维数灾难**(curse of dimensionality)

> 缓解维数灾难有两个重要的途径：一是降维，二是特征选择.

本篇笔记主要讨论的是**降维**.

***

## 经典的降维方法-理论部分

### 多维缩放(Multiple Dimensional Scaling,MDS)

- principle: 要求原始高维空间中样本的距离在降维到低维空间中仍然得以保持


- 假设前提：

          1.原高维空间为d,样本集上有m个样本，记作向量x。样本间的距离构成了一个(m * m)的距离矩阵D。

          2.降维后的空间为d'，m个样本在此空间上的表示记为向量z。


在笔记本上有关于MDS的公式推导，在此blog中浓缩为**算法**步骤如下：

    1.从原始高维空间中的距离矩阵D出发，为保证高维到低维样本距离不变性，推导出低维空间d'中向量z的内积矩阵B应有的形式。简而言之，从D计算出B。

    2.对内积矩阵B作特征分解，取出d'(<d)个最大的特征值构成新的对角矩阵，并取出相应的d'个特征向量构成新的特征向量矩阵。从这一步可以体会到，虽然MDS是从**距离不变性**出发导出了B矩阵（此时还并未达到降维的效果）。但在步骤二，仅提取出d'个最大特征值已经损失了这种不变性。

    3.根据新的对角矩阵和特征向量矩阵表示出低维空间的Z矩阵（即m个z向量）

至此，MDS实现了降维。


- 理解：

某种程度上说，降维就是要把对一个事物的描述所使用的feature减少，最理想的情况是，即使减少了描述事物的feature数目，仍然能够清晰无误地表示出该对象。

而最简单的降维方式，就是对高维空间进行**线性变换**。线性变换也可以理解为：我们试图寻找一个更适合表示样本的坐标系，在这种新的坐标系中，我们人为地忽略一些坐标，不会对描述这个样本产生很大的影响。当我们忽略了新坐标系中的一些坐标时，也可以理解为，我们省去了新的坐标系中的一些坐标轴，于是得到一个低维空间。**省略后的坐标系是由取出的d'个特征向量搭建的。**

线性变化中有一种非常特殊的变换：**正交变换**

理解正交变换其实是很直观的。我们说MDS寻找的坐标系，是最能表达原有样本的d'个特征向量搭建的。但它并没有显示地要求低维空间中的坐标系是一个正交坐标系。MDS是单纯地从**距离不变**来导出的B矩阵，至于后续对B矩阵作特征分解，再提取出d'个特征向量。这已是后话。当然，特征分解已经保证了新的坐标轴之间的正交性。

坐标轴之间如果是相互正交的，可想而知，每个坐标能够最大程度无冗余地表示原样本某个维度上的信息，因此，我们人为地忽略掉一些不重要的小坐标，因坐标轴之间的独立性，不会对原样本的表达产生多大的影响。这是正交变换非常直观的好处。


***

### 主成分分析（Principal Component Analysis,PCA)

承接上面所说，进行线性变换再人为丢弃掉一些坐标轴可以得到一个新的低维子空间。

我们对低维子空间的期待和要求，决定了算法中:

     使用什么样的矩阵做线性变换；
     线性变换后以什么样的原则来丢弃坐标轴（或坐标）


首先，我们希望中的低维子空间是什么样的呢？

如果是针对分类问题，那么自然地，我们希望生成的这个低维子空间能够对样本具有最大的可分性。

**什么是最大的可分性？用什么数学语言来描述这种可分性？并且要通过这个可分性的定义去反推一个适合的线性变换算子。这些都是PCA的算法中需要阐明的。**


- 最大可分性：在低维空间中定义一个超平面

在SVM中，我们已经体会过什么是超平面。如果仍然觉得抽象的话，不妨就理解成二维空间中一条区分样本的直线。我们对这条直线的期待是：它能够尽量正确地区分出原样本投影到直线上的值（之所以是值，是因为投影到这条直线上后原二维坐标变为了一个一维的值）.

推广到超平面上，我们的期待陈述为：希望样本在这个超平面上的投影能够尽量地区分开。

除了可分性之外，为什么还要对超平面提出一个**最近重构性**的要求呢？


- 最近重构性：样本点到这个超平面的距离都足够近

这个性质看起来像是一种偏好，如果能够实现最大可分性的线性变换矩阵有很多个，那么从其中挑选出满足**最近重构性**的一个作为最终的变换矩阵。

(comment:**数学推导证明，这不是一种偏好，而是对“最大可分性”的另一种等价表述。这意味着：实现最大可分性的线性变换矩阵同时也满足了最近重构性，所以它不是一种基于实现最大可分之上的偏好。上段话保留了我最初稍显偏颇的理解，是为了让自己感受思维的过程**)

数学推导见笔记本。

- 理解：

**PART ONE PCA的总体特征**

PCA既然作为线性变换降维的一种方式，也就是说它可以被写为Z = W_t * X的形式。

理解变换矩阵W很重要：

最初设想变换矩阵是一个（d * d)的矩阵，也就是说，它包含d个d维基向量。由此可见，变换矩阵中的每一个向量是d维的，和原始空间的维数是一致的。

PCA中使用的变换矩阵是一个单位正交矩阵。正是这个特性赋予了PCA很多可解释的意义，在数学推导中体现为有些项为1，可以作为常数在优化问题时不纳入考虑。

注意此时的W矩阵是没有降维效果的，它仅仅是实现了原空间坐标系的一个正交变换。

真正地降维发生在：舍弃W矩阵中的一些基向量，留下d'个构建成一个正交变换后的降维矩阵（d * d')。

**PART TWO 从两个角度来理解PCA中W的计算**

1.最近重构性

顾名思义，从重构的角度来理解：我们希望在低维的子空间，如果要根据坐标（z，数值）和坐标轴（w向量）来重构样本，重构出的样本和原始样本的距离最小（以欧式距离为度量的误差）

为什么会有误差，是因为变换矩阵W本来是一个不损失维度的单位正交阵，但是在投影之后，我们舍弃了一些坐标，这相当于我们用了一个残缺的单位正交阵来进行线性变换，得到了一个低维的子空间。所以根据残缺的坐标系和坐标，重构出的样本必然也是残缺的。然而，本质上，我们希望这种残缺尽量小。

于是数学公式上我们表示出这个距离度量，最小化该距离，同时加入对W的单位正交要求构造出一个带**等式约束的优化问题**.

2.最大可分性

另一方面，从最大可分性的角度，我们不再考虑要用投影后的坐标去重建一个原始的样本。我们仅关注这些投影后的坐标。

对这些坐标我们的期待是：它们能够尽量被区分开。这样的一个概念被数学建模为：这些投影后的坐标**方差最大**。

其实，投影后的坐标方差最大，代表的是它们偏离均值的程度。而因为一开始我们对原始样本做了中心化处理，使得这些样本的均值为0，于是投影点的方差最大化，被奇妙地等效为了另一个优化问题：目标函数是最大化投影后的向量的内积（模的平方），约束仍然是变换矩阵的特性。这个优化问题是**完全等价于最近重构性推导出的优化问题的**.

这真的很奇妙，不禁想问：

- 为什么基于投影坐标重构的样本，要求它和原样本的误差最小，从数学上完全等价于要求投影坐标的方差最大化呢？

这里我们假设一些条件：1.变换矩阵是单位正交的；
                      2.样本均值为0。（中心化）

诚然，这些条件是导致这两者等价的原因。这似乎在告诉我们：在空间中的正交投影是使得投影前后的欧氏距离最小的一种方式，同时，它也会使投影后的坐标最为分散。最为分散意味着，重构更加容易，而重构更加容易意味着，投影前后的误差最小。

所以这其实是我们自然语言在表达同一本质事物时采用的不同的描述方式，之所以我们发现我们所说的不外乎同一件事，是因为数学公式（优化问题）告诉我们：

"你们所说的，其实是一样的啊！"


- 紧接着还有更奇妙的一件事

对上述优化问题用拉格朗日方法进行求解，会发现，变换矩阵中每个基向量的表示，就是对**原样本的协方差矩阵**进行特征值分解的特征向量！！！

回顾我之前对PCA的理解一直都是：对样本的协方差矩阵进行特征分解，取出最大的d'个特征值对应的特征向量，这d'个特征向量就是低维子空间的坐标系。

于是此刻，我终于知道了为什么要通过对原始样本的协方差矩阵来进行特征分解得到低维空间的坐标。

协方差矩阵一直很困惑我，为什么是对原样本的协方差矩阵做特征分解呢？

现在我终于可以回答，协方差矩阵是如何出现的。可以说它缘起于从投影后坐标方差最大化的表示中引入的，也可以说它缘起于最近重构性中的距离度量的表示中引入的。

而这件奇妙的事也一并解了我对MDS算法中为什么要从**原样本的距离矩阵**算到**低维空间的内积矩阵**的困惑。

当然本质上来说，MDS中降维后的内积矩阵的出现是因为使用了距离度量来作为优化目标。
但是我们同时可以发现，**原样本的协方差矩阵**和**降维后样本的内积矩阵**是有某种内在的联系的。

经过数学公式的书写发现，在满足变换矩阵是正交的这一条件下，对降维后样本的内积矩阵进行特征分解完全等价于样本的协方差矩阵的特征分解。

也就是说：

> 如果在MDS中从D矩阵对到B矩阵时引入正交性约束，那么得到的就是PCA一样的结果！
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    

***

## Sklearn中的降维方法-Coding部分
## PCA
- 根据user guide，PCA作为一个transformer object，利用fit method来学习n个成分(components).相当于找出了n个特征向量。对于一个新的样本，将它投影到已知的这n个特征向量上，即实现了降维。

- 参数“whiten=True"：白化，意味着将每一个成分限制在方差为1的范围内，把样本投影到奇异空间(singular space)中。为什么要白化？

> This is often useful if the models down-stream make strong assummption on the isotropy of the signal:this is for example the case for Support Vector Machines with the RBF kernel and the K-Means clustering algorithm.

白化适用于当我们的模型对数据作出了isotropy的假设（即数据集的方差相同，各向同性），比如使用了RBF核的支持向量机和K-means聚类算法。

- PCA object也提供了一个概率的解释，基于投影后方差最大化给出了原数据的似然概率。
于是乎，PCA可以使用一种打分机制(score method)来进行交叉验证。

### **Demo1- 比较PCA-2D和线性判决分析（LDA)对同一组数据集降维**

- 回顾LDA

在sklearn的class LinearDiscriminantAnalysis中是这样介绍LDA的：

> a classifier with a linear decision boundary,generated by fitting class conditional densities to the data and using Bayes'rule.

LDA是一个拥有线性决策边界的分类器，它的决策边界是通过使用贝叶斯定律计算样本的类条件概率得到的。

这句话给我们几点启示：

     1.LDA 是一个分类器，用于分类任务；
     2.LDA 是监督学习方法，它会利用已有的labels来计算类条件概率
     3.LDA 通过降维来分类，所以LDA的学习过程就是降维过程。

> the model fits a Gaussian density to each class,assuming that all classes share the same covariance matrix

这句话是说，LDA生成的模型认为，每一个类别中的样本都是高斯分布的，并且有相同的协方差矩阵。这意味着，整体的数据集是isotrophic的。

- LDA 与 PCA-2D降维的不同

从demo的演示结果中我们可以看见，这两种方法对于同一个数据集的降维效果是不同的。虽然demo中并没有对降维的性能做一个度量，很可能是这两种方法本质上适用的是不同的情况，所以并没有孰优孰劣之分，所以我们仅就它们的差异来做一个总结：

1.适用场景不同：LDA是监督降维技术，PCA-2D是非监督降维。

2.出发点不同：

    LDA是作为线性分类器的模型提出的，它本质的思想是希望把样本投影在另一个空间后，能够实现分类任务，即使同一类的样本尽量近，而异类的样本尽量远。

    PCA是直接作为一种降维技术提出的，它的本质是通过线性变换将原样本用新的坐标系表示，然后舍掉一些不重要的坐标轴构造出一个新的坐标空间。由于PCA更像是在真正实现分类任务之前做的一个预处理，它的最终目的仍然是针对实现更好的分类，所以PCA希望这个降维后的空间能够实现样本的“最大可分性”或者说“最近重构性”，于是看起来PCA和LDA在数学公式的表示中才会呈现出某些殊途同归。

总结起来，虽然LDA和PCA的出发点不同，但它们却有着相同的目标：就是实现样本的正确划分。并且它们都是通过线性变换投影来实现，所以LDA和PCA又有着很多共通之处。
PCA之所以可以避免使用labels，是因为它直接对**原样本的协方差矩阵进行特征分解**，而LDA中特征分解的对象没那么“纯粹”：它把样本按照不同的类别信息做了划分，然后计算类内(within-class)散度矩阵和类间(between-class)散度矩阵，对这**两个矩阵做广义的特征分解**。

需要特别分析的是：散度矩阵和协方差矩阵两者该如何理解？直觉上它们好像表达的意思有重叠性？

散度可想而知，描述了一个集合的分散程度；协方差，其实也是描述的一组随机变量的分散程度。那么为什么要新提出一个散度的概念呢？

这是因为，协方差围绕的中心是样本的均值。而散度的中心则是根据我们自定义的，比如一个类别的中心。所以，散度其实是协方差的一种推广。

## Incremental PCA 增强的PCA

- 传统PCA的局限在哪里？

> The biggest limitation is that PCA only supports batch processing,which means all of the data to be processed must fit in main memory.

传统PCA只支持batch process，这意味着我们要降维的原始数据必须都在主存中处理。

> Incremental PCA 是一种新的计算结构，算法并没有改变。它使用partial_fit method每次仅计算一部分数据(a chunk of data fetched sequentially from the local hard driver or a network database)

可以把Incremental PCA看做一种串行的不完全计算，它旨在解决输入的数据集太大无法全部放在内存中的问题。

所以可以把IPCA理解为一种mini-batch 模式。

## Approximate PCA 渐近PCA

**实际中常通过对样本矩阵进行奇异值分解来替代样本协方差矩阵的特征分解**

- 例子：人脸识别

如果我们的数据集是这样的一些样本，每一个样本都是一个64*64的灰度图像，也就是说，每个样本的维度是4096！要是用RBF的SVM来训练这个分类器，可想而知是多么的恐怖。
进一步，如果我们知道“the samples lie on a manifold of much lower dimension",比如200维。那么使用PCA就可以达成将这些数据线性变换降维同时"preserve most of the explained variance"

class RandomizedPCA对于这类问题非常有用！

考虑：既然最后我们会丢掉大部分的奇异向量，那么有没有一种更高效地方法，通过对要保留的奇异向量进行渐近估计，于是我们就只算这些要保留的奇异向量。而不是把所有的奇异向量都算出来后再进行丢弃。

> RandomizedPCA can hence be used as a drop in replacement for PCA with the exception that we need to give it the size of the low-dimensional space n_components as a mandatory input parameter.

## Kernel PCA 核主成分分析

有种感觉，降维和监督学习中SVM方法有非常紧密的联系。降维也分线性降维和利用核方法的非线性降维，这和SVM如出一辙。我感觉SVM就像是旨在实现降维后稀疏表示的分类器。



> 线性降维是通过从高维空间到低维空间的线性映射实现的，然而实际中，有些情况需要非线性映射才能找到恰当的低维嵌入。

这句话怎么理解呢？

这样的一种非线性方式的降维，竟然需要先使用**核化**技术将样本映射到一个高维的特征空间，再从特征空间施行PCA进行降维。

仿佛绕了一圈：如果说原本样本处在一个高维的样本空间，我们希望降维使样本映射到一个更低维的子空间，而如果直接使用线性映射，我们得不到想要的那个本征低维空间(intrinsic)，非得先把样本通过核化映射到一个高维的特征空间，再从这个高维的特征空间来降维才行。

于是我产生了几个疑问：

    1.什么样的情况我们知道，线性降维无法得到intrinsic的低维空间呢？

    2.高维的样本空间到高维的特征空间，维度的变化大概是如何的？

我想，这个问题的解决就是要弄清楚**核化**究竟是个什么操作？它不仅出现在降维中，更是SVM的精髓。

鉴于本篇博文已经太长，遂另起一篇，专门讨论“核化”。[kernel method]()



## 后话：一些问题

**Q1**: 我们是否总是希望“密采样”呢？渴望一个简单的分类器(1NN)在分类问题上的泛化误差不要太大时，我们推导出了“密采样”的前提。然而这个情况是普适的吗？也就是说，无论使用什么模型，“密采样”都能提升模型的性能吗？

目前我的回答是：应该是的，密采样意味着用于训练的数据愈多，按理说得到的模型的泛化能力应该更强。
