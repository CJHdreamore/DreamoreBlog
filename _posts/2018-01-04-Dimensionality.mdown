---
layout: post
title: 机器学习中的降维
author: 陈小耗
data: 2018-01-04
categories: learning
tags: [西瓜书,sklearn]
---
# 前言：这篇笔记是结合周志华老师的西瓜书以及sklearn的user guide学习**降维算法**的过程

***

## 为什么要讨论降维？

**西瓜书中是通过KNN（K-近邻算法）来引入的维数问题。**

- K-NN算法是一种懒惰学习（lazy learning)

在训练过程中，其实它只保存了样本。在测试时，KNN计算测试样本和每一个训练样本间的距离，在选定K的情况下，返回K个最近的训练样本用于分类或者回归。

K的选定是最后分类或者回归结果好坏的一个重要参数。

在书中讨论了K=1时，即最近邻分类器在分类时的泛化误差，一个简单推导后得出的结论是：

> 最简单的1NN的泛化错误率不超过贝叶斯最优分类器的错误率的两倍！！


然而，得到这个结论有一个前提假设：**对于任意的测试样本，总能在其任意近的范围内找到一个训练样本**

这个假设的通俗解释就是：我们从总体中采样得到的训练样本密度必须足够大！也就是所谓的**dense sample**。

实际中满足密采样是怎么一回事呢？

举个例子：想要在一个维度=1的测试样本(只有一个属性)附近0.001的距离范围内都能找到一个训练样本，那么就要求1000个样本点平均分布在归一化后的该属性的取值范围内。

倘若有更多的属性（样本是高维的）,那么所需的样本将会呈指数上升，比如维度=20，需要样本1000^20.更遑论之后还要对这些样本进行距离的计算。

也就是说：在高维情况下，会存在样本稀疏、距离计算困难这样的问题。这就是机器学习中的**维数灾难**(curse of dimensionality)

> 缓解维数灾难有两个重要的途径：一是降维，二是特征选择.

本篇笔记主要讨论的是**降维**.


## 经典的降维方法-理论部分

### 多维缩放(Multiple Dimensional Scaling,MDS)

- principle: 要求原始高维空间中样本的距离在降维到低维空间中仍然得以保持

假设前提：1.原高维空间为d,样本集上有m个样本，记作向量x。样本间的距离构成了一个(m * m)的距离矩阵D。

          2.降维后的空间为d'，m个样本在此空间上的表示记为向量z。


在笔记本上有关于MDS的公式推导，在此blog中浓缩为算法步骤如下：

1.从原始高维空间中的距离矩阵D出发，为保证高维到低维样本距离不变性，推导出低维空间d'中向量z的内积矩阵B应有的形式。简而言之，从D计算出B。

2.对内积矩阵B作特征分解，取出d'(<d)个最大的特征值构成新的对角矩阵，并取出相应的d'个特征向量构成新的特征向量矩阵。从这一步可以体会到，虽然MDS是从**距离不变性**出发导出了B矩阵，但在这一步，仅提取出d'个最大特征值已经损失了这种不变性。

3.根据新的对角矩阵和特征向量矩阵表示出低维空间的Z矩阵（即m个z向量）

至此，MDS实现了降维。


- 理解：

某种程度上说，降维就是要把对一个事物的描述所使用的feature减少，最理想的情况是，即使减少了描述事物的feature数目，仍然能够清晰无误地表示出该对象。

而最简单的降维方式，就是对高维空间进行线性变换，线性变换也可以理解为：我们试图寻找一个更适合表示样本的坐标系，在这种新的坐标系中，用来表达一个样本的属性的数目可以尽量地小。MDS提取最大的d'个特征值，其实就是在人为地减少表示样本的属性，从而构造出一个新的坐标系。**这个新的坐标系就是取出的d'个特征向量**

线性变化中有一种非常特殊的变换：**正交变换**

理解正交变换其实是很直观的。我们说MDS寻找的坐标系，是最能表达原有样本的d'个特征向量搭建的。但实际上在这里是直接去掉了(d-d')个坐标分量，它们并不一定为0，所以信息是损失了的。

然而正交变换（如果可以找到的话）,是没有损失信息的。它实现降维的原理是：原有空间中表示样本的坐标系是一个冗余的非正交的坐标系，因此一个样本需要很多属性来表达。而在经过正交变换之后，新的空间中的坐标系是正交的，每个属性能够最大程度地表达样本，而不冗余。因此也就减少了表示一个样本所需要的feature，从而实现了降维。



- 问题：是否每一个原有的空间，都能做正交变换找到一个新的低维子空间呢？

        正交变换后的低维空间和MDS得到的低维空间，又有何区别?不同的降维方法适用于哪些不同的场景？

***

### 主成分分析（Principal Component Analysis,PCA)

承接上面所说，进行线性变换可以得到一个新的低维子空间。

我们对低维子空间的期待和要求，决定了使用什么样的矩阵做此线性变换。

那么，我们希望中的低维子空间是什么样的呢？

如果是针对分类问题，那么自然地，我们希望生成的这个低维子空间能够对样本具有最大的可分性。

**什么是最大的可分性？用什么数学语言来描述这种可分性？并且要通过这个可分性的定义去反推一个适合的线性变换算子。这些都是PCA的算法中需要阐明的。**


- 最大可分性：在低维空间中定义一个超平面

在SVM中，我们已经体会过超平面是什么。如果仍然觉得抽象的话，不妨就理解成二维空间中一条区分样本的直线。我们对这条直线的期待是：它能够尽量正确地区分出不同类的样本。（其实也就是二维的样本投影到这根直线上变为一个一维的点后，这些值是不同的，故可分）.

推广到超平面上，我们的期待陈述为：希望样本在这个超平面上的投影能够尽量地区分开。

除了可分性之外，为什么还要对超平面提出一个**最近重构性**的要求呢？


- 最近重构性：> 样本点到这个超平面的距离都足够近

这个性质看起来像是一种偏好，如果能够实现最大可分性的线性变换矩阵有很多个，那么从其中挑选出满足**最近重构性**的一个作为最终的变换矩阵。

数学推导见笔记本。

- 理解：

**PART ONE PCA的总体特征**

PCA既然作为线性变换来降维的一种方式，也就是说它可以被写为Z = W_t * X的形式。

理解变换矩阵W很重要：变换矩阵是一个（d * d')的矩阵，也就是说，它包含d'个d维基向量。由此可见，变换矩阵中的每一个向量是d维的，和原始空间的维数是一致的。

另一方面，PCA中使用的变换矩阵是一个单位正交矩阵。正是这个特性赋予了PCA很多可解释的意义，在数学推导中体现为有些项为1，可以作为常数在优化问题时不纳入考虑。

**PART TWO 从两个角度来理解PCA中W的计算**

1.最近重构性

顾名思义，从重构的角度来理解：我们希望在低维的子空间，如果要根据坐标（z，数值）和坐标系（w向量）来重构样本，重构出的样本和原始样本的距离最小（以欧式距离为度量的误差）

为什么会有误差，是因为变换矩阵W本来是一个d维的单位正交阵，但是在投影之后，我们舍弃了一些坐标，这相当于我们用了一个残缺的单位正交阵来进行线性变换，得到了一个低维的子空间。所以根据残缺的坐标系和坐标，重构出的样本必然也是残缺的。然而，本质上，我们希望这种残缺尽量小。

于是数学公式上我们表示出这个距离度量，最小化该距离，同时加入对W的单位正交要求构造出一个带**等式约束的优化问题**.

2.最大可分性

另一方面，从最大可分性的角度，我们不再考虑要用投影后的坐标去重建一个原始的样本。我们仅关注这些投影后的坐标。

对这些坐标我们的期待是：它们能够尽量被区分开。这样的一个概念被数学建模为：这些投影后的坐标方差要最大。

其实，投影后的坐标方差最大，代表的是它们偏离均值的程度。而因为一开始我们对原始样本做了中心化处理，使得这些样本的均值为0，于是投影点的方差最大化，被奇妙地等效为了另一个优化问题：目标函数是最大化投影坐标的方差，约束仍然是变换矩阵的特性。这个优化问题是**完全等价于最近重构性推导出的优化问题的**.

这真的很奇妙，不禁想问：

- 为什么基于投影坐标重构的样本，要求它和原样本的误差最小，从数学上完全等价于要求投影坐标的方差最大化呢？

这里我们假设一些条件：1.变换矩阵是单位正交的；
                      2.样本均值为0。（中心化）

诚然，这些条件是导致这两者等价的原因。这似乎在告诉我们：在空间中的正交投影是使得投影前后的欧氏距离最小的一种方式，同时，它也会使投影后的坐标最为分散。最为分散意味着，重构更加容易，而重构更加容易意味着，投影前后的误差最小。

所以这其实是我们自然语言在表达同一本质事物时才用的不同的描述方式，之所以我们发现我们所说的不外乎同一件事，是因为数学公式（优化问题）告诉我们：

"你们所说的，其实是一样的啊！"


- 紧接着还有更奇妙的一件事

对上述优化问题用拉格朗日方法进行求解，会发现，变换矩阵中每个基向量的表示，就是对原样本的协方差矩阵进行特征值分解的特征向量！！！

所以我之前对PCA的理解一直都是：对样本的协方差矩阵进行特征分解，取出最大的d'个特征值对应的特征向量，这d'个特征向量就是低维子空间的坐标系。

于是此刻，我终于知道了为什么要通过对原始样本的协方差矩阵来进行特征分解得到低维空间的坐标。

协方差矩阵一直很困惑我，为什么是对原样本的协方差矩阵做特征分解呢？

现在我终于可以回答，协方差矩阵是如何出现的。可以说它缘起于从投影后坐标方差最大化的表示中引入的，也可以说它缘起于最近重构性中的距离度量的表示中引入的。

而这件奇妙的事也一并解了我对MDS算法中为什么要从距离矩阵算到内积矩阵的困惑。

当然本质上来说，MDS中降维后的内积矩阵的出现是因为使用了距离度量来作为优化目标。
但是我们同时可以发现，**原样本的协方差矩阵**和**降维后样本的内积矩阵**是有某种内在的联系的。

经过数学公式的书写发现，在满足变换矩阵是正交的这一条件下，对降维后样本的内积矩阵进行特征分解完全等价于样本的协方差矩阵的特征分解。

也就是说：

> 如果在MDS中从D矩阵对到B矩阵时引入正交性约束，那么得到的就是PCA一样的结果！
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    






